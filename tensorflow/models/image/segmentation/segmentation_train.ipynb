{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import segmentation_input\n",
    "import segmentation_model\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_TO_RECORD = os.path.join(segmentation_input.DIRECTORY,segmentation_input.TF_RECORDS)\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = segmentation_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN\n",
    "NUM_EPOCHS_PER_DECAY = 10\n",
    "INITIAL_LEARNING_RATE = 1.0E-1\n",
    "LEARNING_RATE_DECAY_FACTOR = 0.1\n",
    "MOVING_AVERAGE_DECAY = 0.9\n",
    "STEPS_PER_DISPLAY = 10\n",
    "STEPS_PER_SUMMARY = 100\n",
    "STEPS_PER_CHECKPT = 1000\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', 'segmentation_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1500,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_size', 32,\n",
    "                            \"\"\"Batch size.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _add_loss_summaries(total_loss):\n",
    "    \"\"\"Add summaries for losses in CIFAR-10 model.\n",
    "    Generates moving average for all losses and associated summaries for\n",
    "    visualizing the performance of the network.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "    Returns:\n",
    "        loss_averages_op: op for generating moving averages of losses.\n",
    "    \"\"\"\n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "    loss_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, name='avg')\n",
    "    losses = tf.get_collection('losses')\n",
    "    loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "\n",
    "    # Attach a scalar summary to all individual losses and the total loss; do the\n",
    "    # same for the averaged version of the losses.\n",
    "    for l in losses + [total_loss]:\n",
    "        # Name each loss as '(raw)' and name the moving average version of the loss\n",
    "        # as the original loss name.\n",
    "        tf.scalar_summary(l.op.name +' (raw)', l)\n",
    "        tf.scalar_summary(l.op.name, loss_averages.average(l))\n",
    "\n",
    "    return loss_averages_op\n",
    "\n",
    "def train_helper(total_loss, global_step):\n",
    "    \"\"\"Train segmentation model.\n",
    "    Create an optimizer and apply to all trainable variables. Add moving\n",
    "    average for all trainable variables.\n",
    "    Args:\n",
    "        total_loss: Total loss from loss().\n",
    "        global_step: Integer Variable counting the number of training steps\n",
    "          processed.\n",
    "    Returns:\n",
    "        train_op: op for training.\n",
    "    \"\"\"\n",
    "    # Variables that affect learning rate.\n",
    "    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size\n",
    "    decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "    # Decay the learning rate exponentially based on the number of steps.\n",
    "    lr = tf.train.exponential_decay(INITIAL_LEARNING_RATE,\n",
    "                                    global_step,\n",
    "                                    decay_steps,\n",
    "                                    LEARNING_RATE_DECAY_FACTOR,\n",
    "                                    staircase=True)\n",
    "    tf.scalar_summary('learning_rate', lr)\n",
    "\n",
    "    # Generate moving averages of all losses and associated summaries.\n",
    "    loss_averages_op = _add_loss_summaries(total_loss)\n",
    "\n",
    "    # Compute gradients.\n",
    "    with tf.control_dependencies([loss_averages_op]):\n",
    "        opt = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads = opt.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n",
    "\n",
    "    # Add histograms for trainable variables.\n",
    "    for var in tf.trainable_variables():\n",
    "        tf.histogram_summary(var.op.name, var)\n",
    "\n",
    "    # Add histograms for gradients.\n",
    "    for grad, var in grads:\n",
    "        if grad is not None:\n",
    "            tf.histogram_summary(var.op.name + '/gradients', grad)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(\n",
    "        MOVING_AVERAGE_DECAY, global_step)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Graph().as_default():\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        images, labels = segmentation_input.distorted_inputs([PATH_TO_RECORD],FLAGS.batch_size)\n",
    "\n",
    "        # Build a Graph that computes the logits predictions from the\n",
    "        # inference model.\n",
    "        logits = segmentation_model.inference(images,is_train=True)\n",
    "\n",
    "        # Calculate loss.\n",
    "        loss,acc = segmentation_model.loss_and_accuracy(logits, labels)\n",
    "        \n",
    "        # updates the model parameters.\n",
    "        train_op = train_helper(loss, global_step)\n",
    "\n",
    "        # Create a saver.\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Build the summary operation based on the TF collection of Summaries.\n",
    "        summary_op = tf.merge_all_summaries()\n",
    "\n",
    "        # Build an initialization operation to run below.\n",
    "        init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Start running operations on the Graph.\n",
    "        sess = tf.Session()\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the queue runners.\n",
    "        tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "        summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n",
    "        \n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "            _, loss_value, acc_value = sess.run([train_op, loss, acc])\n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "            if step % STEPS_PER_DISPLAY == 0:\n",
    "                num_examples_per_step = FLAGS.batch_size\n",
    "                examples_per_sec = num_examples_per_step / duration\n",
    "                sec_per_batch = float(duration)\n",
    "\n",
    "                format_str = ('%s: step %d, loss = %.2f, accuracy = %.2f, (%.1f examples/sec; %.3f '\n",
    "                              'sec/batch)')\n",
    "                print (format_str % (datetime.now(), step, loss_value, acc_value,\n",
    "                        examples_per_sec, sec_per_batch))\n",
    "                \n",
    "            if step % STEPS_PER_SUMMARY == 0:\n",
    "                summary_str = sess.run(summary_op)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "            # Save the model checkpoint periodically.\n",
    "            if step % STEPS_PER_CHECKPT == 0 or (step + 1) == FLAGS.max_steps:\n",
    "                checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step=step)\n",
    "        \n",
    "        tf.train.write_graph(sess.graph_def, FLAGS.train_dir, \"segmentation.pb\", False) #proto\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 3198 images before starting to train. This will take a few minutes.\n",
      "2016-10-10 11:48:24.053114: step 0, loss = 1.39, accuracy = 0.25, (13.9 examples/sec; 2.301 sec/batch)\n",
      "2016-10-10 11:48:31.150984: step 10, loss = 1.39, accuracy = 0.26, (60.3 examples/sec; 0.531 sec/batch)\n",
      "2016-10-10 11:48:36.842344: step 20, loss = 1.39, accuracy = 0.26, (49.2 examples/sec; 0.651 sec/batch)\n",
      "2016-10-10 11:48:42.350301: step 30, loss = 1.39, accuracy = 0.27, (57.6 examples/sec; 0.556 sec/batch)\n",
      "2016-10-10 11:48:47.987727: step 40, loss = 1.39, accuracy = 0.27, (57.0 examples/sec; 0.561 sec/batch)\n",
      "2016-10-10 11:48:53.832714: step 50, loss = 1.39, accuracy = 0.28, (56.7 examples/sec; 0.564 sec/batch)\n",
      "2016-10-10 11:48:59.660985: step 60, loss = 1.39, accuracy = 0.29, (55.0 examples/sec; 0.582 sec/batch)\n",
      "2016-10-10 11:49:05.552546: step 70, loss = 1.39, accuracy = 0.30, (54.5 examples/sec; 0.587 sec/batch)\n",
      "2016-10-10 11:49:11.381988: step 80, loss = 1.39, accuracy = 0.31, (55.6 examples/sec; 0.576 sec/batch)\n",
      "2016-10-10 11:49:17.406878: step 90, loss = 1.38, accuracy = 0.34, (55.7 examples/sec; 0.574 sec/batch)\n",
      "2016-10-10 11:49:23.297580: step 100, loss = 1.38, accuracy = 0.37, (55.0 examples/sec; 0.582 sec/batch)\n",
      "2016-10-10 11:49:29.712456: step 110, loss = 1.38, accuracy = 0.39, (55.2 examples/sec; 0.580 sec/batch)\n",
      "2016-10-10 11:49:35.643394: step 120, loss = 1.37, accuracy = 0.40, (47.4 examples/sec; 0.675 sec/batch)\n",
      "2016-10-10 11:49:41.377009: step 130, loss = 1.36, accuracy = 0.47, (55.6 examples/sec; 0.576 sec/batch)\n",
      "2016-10-10 11:49:47.032874: step 140, loss = 1.31, accuracy = 0.52, (56.1 examples/sec; 0.570 sec/batch)\n",
      "2016-10-10 11:49:52.753910: step 150, loss = 1.29, accuracy = 0.44, (54.4 examples/sec; 0.588 sec/batch)\n",
      "2016-10-10 11:49:58.490970: step 160, loss = 1.20, accuracy = 0.48, (56.9 examples/sec; 0.562 sec/batch)\n",
      "2016-10-10 11:50:04.433509: step 170, loss = 1.18, accuracy = 0.46, (55.9 examples/sec; 0.573 sec/batch)\n",
      "2016-10-10 11:50:10.162522: step 180, loss = 1.09, accuracy = 0.52, (48.2 examples/sec; 0.664 sec/batch)\n",
      "2016-10-10 11:50:15.803831: step 190, loss = 1.08, accuracy = 0.53, (56.9 examples/sec; 0.562 sec/batch)\n",
      "2016-10-10 11:50:21.483249: step 200, loss = 1.01, accuracy = 0.55, (56.4 examples/sec; 0.568 sec/batch)\n",
      "2016-10-10 11:50:27.855168: step 210, loss = 1.01, accuracy = 0.54, (54.9 examples/sec; 0.583 sec/batch)\n",
      "2016-10-10 11:50:33.864799: step 220, loss = 0.94, accuracy = 0.56, (53.9 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 11:50:40.202779: step 230, loss = 0.95, accuracy = 0.55, (44.3 examples/sec; 0.722 sec/batch)\n",
      "2016-10-10 11:50:46.089740: step 240, loss = 0.85, accuracy = 0.60, (56.4 examples/sec; 0.567 sec/batch)\n",
      "2016-10-10 11:50:51.776921: step 250, loss = 0.90, accuracy = 0.55, (56.2 examples/sec; 0.570 sec/batch)\n",
      "2016-10-10 11:50:57.544878: step 260, loss = 0.87, accuracy = 0.59, (56.2 examples/sec; 0.570 sec/batch)\n",
      "2016-10-10 11:51:03.295655: step 270, loss = 0.89, accuracy = 0.57, (56.0 examples/sec; 0.572 sec/batch)\n",
      "2016-10-10 11:51:09.144320: step 280, loss = 0.91, accuracy = 0.60, (50.0 examples/sec; 0.640 sec/batch)\n",
      "2016-10-10 11:51:15.032613: step 290, loss = 0.80, accuracy = 0.61, (55.6 examples/sec; 0.575 sec/batch)\n",
      "2016-10-10 11:51:20.931216: step 300, loss = 0.82, accuracy = 0.59, (55.1 examples/sec; 0.580 sec/batch)\n",
      "2016-10-10 11:51:27.404593: step 310, loss = 0.86, accuracy = 0.57, (54.7 examples/sec; 0.585 sec/batch)\n",
      "2016-10-10 11:51:33.122116: step 320, loss = 0.84, accuracy = 0.58, (56.8 examples/sec; 0.564 sec/batch)\n",
      "2016-10-10 11:51:38.884465: step 330, loss = 0.79, accuracy = 0.58, (55.2 examples/sec; 0.580 sec/batch)\n",
      "2016-10-10 11:51:44.699088: step 340, loss = 0.83, accuracy = 0.58, (55.1 examples/sec; 0.580 sec/batch)\n",
      "2016-10-10 11:51:50.498619: step 350, loss = 0.77, accuracy = 0.62, (56.5 examples/sec; 0.567 sec/batch)\n",
      "2016-10-10 11:51:56.347958: step 360, loss = 0.76, accuracy = 0.60, (55.0 examples/sec; 0.582 sec/batch)\n",
      "2016-10-10 11:52:02.174160: step 370, loss = 0.78, accuracy = 0.58, (53.4 examples/sec; 0.599 sec/batch)\n",
      "2016-10-10 11:52:08.130072: step 380, loss = 0.77, accuracy = 0.60, (55.6 examples/sec; 0.575 sec/batch)\n",
      "2016-10-10 11:52:13.923394: step 390, loss = 0.81, accuracy = 0.57, (55.0 examples/sec; 0.582 sec/batch)\n",
      "2016-10-10 11:52:19.897763: step 400, loss = 0.82, accuracy = 0.59, (53.2 examples/sec; 0.601 sec/batch)\n",
      "2016-10-10 11:52:26.416204: step 410, loss = 0.74, accuracy = 0.60, (55.6 examples/sec; 0.575 sec/batch)\n",
      "2016-10-10 11:52:32.300979: step 420, loss = 0.77, accuracy = 0.60, (54.1 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:52:38.279097: step 430, loss = 0.82, accuracy = 0.57, (54.1 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 11:52:44.238191: step 440, loss = 0.74, accuracy = 0.61, (54.1 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 11:52:50.196094: step 450, loss = 0.76, accuracy = 0.58, (55.4 examples/sec; 0.578 sec/batch)\n",
      "2016-10-10 11:52:56.075491: step 460, loss = 0.75, accuracy = 0.60, (54.1 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:53:02.036761: step 470, loss = 0.71, accuracy = 0.64, (54.0 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:53:08.035460: step 480, loss = 0.71, accuracy = 0.62, (53.8 examples/sec; 0.595 sec/batch)\n",
      "2016-10-10 11:53:14.032120: step 490, loss = 0.74, accuracy = 0.60, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:53:20.017492: step 500, loss = 0.81, accuracy = 0.56, (53.1 examples/sec; 0.602 sec/batch)\n",
      "2016-10-10 11:53:26.744794: step 510, loss = 0.71, accuracy = 0.61, (51.0 examples/sec; 0.628 sec/batch)\n",
      "2016-10-10 11:53:32.872198: step 520, loss = 0.72, accuracy = 0.60, (53.4 examples/sec; 0.600 sec/batch)\n",
      "2016-10-10 11:53:38.863459: step 530, loss = 0.74, accuracy = 0.61, (53.8 examples/sec; 0.595 sec/batch)\n",
      "2016-10-10 11:53:44.836095: step 540, loss = 0.66, accuracy = 0.64, (53.3 examples/sec; 0.600 sec/batch)\n",
      "2016-10-10 11:53:50.807353: step 550, loss = 0.69, accuracy = 0.63, (53.6 examples/sec; 0.598 sec/batch)\n",
      "2016-10-10 11:53:56.780916: step 560, loss = 0.75, accuracy = 0.60, (54.1 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 11:54:02.744869: step 570, loss = 0.70, accuracy = 0.64, (54.3 examples/sec; 0.589 sec/batch)\n",
      "2016-10-10 11:54:08.681885: step 580, loss = 0.69, accuracy = 0.67, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:54:14.670286: step 590, loss = 0.66, accuracy = 0.69, (53.0 examples/sec; 0.604 sec/batch)\n",
      "2016-10-10 11:54:20.629781: step 600, loss = 0.64, accuracy = 0.74, (54.1 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:54:27.127473: step 610, loss = 0.64, accuracy = 0.74, (52.6 examples/sec; 0.608 sec/batch)\n",
      "2016-10-10 11:54:33.106192: step 620, loss = 0.66, accuracy = 0.73, (52.8 examples/sec; 0.606 sec/batch)\n",
      "2016-10-10 11:54:39.108429: step 630, loss = 0.67, accuracy = 0.72, (54.4 examples/sec; 0.589 sec/batch)\n",
      "2016-10-10 11:54:45.172112: step 640, loss = 0.64, accuracy = 0.71, (49.5 examples/sec; 0.646 sec/batch)\n",
      "2016-10-10 11:54:51.199330: step 650, loss = 0.59, accuracy = 0.75, (53.8 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 11:54:57.178629: step 660, loss = 0.60, accuracy = 0.75, (54.1 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 11:55:03.143328: step 670, loss = 0.62, accuracy = 0.75, (53.5 examples/sec; 0.598 sec/batch)\n",
      "2016-10-10 11:55:09.095416: step 680, loss = 0.55, accuracy = 0.77, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 11:55:15.188013: step 690, loss = 0.60, accuracy = 0.73, (52.9 examples/sec; 0.605 sec/batch)\n",
      "2016-10-10 11:55:21.201729: step 700, loss = 0.58, accuracy = 0.73, (53.5 examples/sec; 0.598 sec/batch)\n",
      "2016-10-10 11:55:28.136634: step 710, loss = 0.57, accuracy = 0.76, (51.2 examples/sec; 0.625 sec/batch)\n",
      "2016-10-10 11:55:34.384786: step 720, loss = 0.62, accuracy = 0.71, (51.9 examples/sec; 0.616 sec/batch)\n",
      "2016-10-10 11:55:40.878317: step 730, loss = 0.54, accuracy = 0.75, (50.5 examples/sec; 0.633 sec/batch)\n",
      "2016-10-10 11:55:46.996121: step 740, loss = 0.52, accuracy = 0.78, (54.1 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:55:53.034226: step 750, loss = 0.53, accuracy = 0.77, (52.9 examples/sec; 0.604 sec/batch)\n",
      "2016-10-10 11:55:58.989720: step 760, loss = 0.51, accuracy = 0.76, (53.8 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 11:56:04.973377: step 770, loss = 0.50, accuracy = 0.78, (52.6 examples/sec; 0.608 sec/batch)\n",
      "2016-10-10 11:56:10.975497: step 780, loss = 0.49, accuracy = 0.78, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 11:56:16.950553: step 790, loss = 0.46, accuracy = 0.79, (52.8 examples/sec; 0.606 sec/batch)\n",
      "2016-10-10 11:56:23.022180: step 800, loss = 0.43, accuracy = 0.82, (52.1 examples/sec; 0.614 sec/batch)\n",
      "2016-10-10 11:56:29.621889: step 810, loss = 0.43, accuracy = 0.80, (52.7 examples/sec; 0.608 sec/batch)\n",
      "2016-10-10 11:56:35.572094: step 820, loss = 0.55, accuracy = 0.75, (54.7 examples/sec; 0.585 sec/batch)\n",
      "2016-10-10 11:56:41.762139: step 830, loss = 0.56, accuracy = 0.74, (49.3 examples/sec; 0.649 sec/batch)\n",
      "2016-10-10 11:56:47.900391: step 840, loss = 0.51, accuracy = 0.77, (54.1 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 11:56:53.987515: step 850, loss = 0.47, accuracy = 0.81, (48.5 examples/sec; 0.659 sec/batch)\n",
      "2016-10-10 11:57:00.541068: step 860, loss = 0.49, accuracy = 0.81, (51.5 examples/sec; 0.621 sec/batch)\n",
      "2016-10-10 11:57:06.866155: step 870, loss = 0.45, accuracy = 0.82, (54.0 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 11:57:12.828233: step 880, loss = 0.51, accuracy = 0.81, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:57:18.898433: step 890, loss = 0.47, accuracy = 0.82, (50.8 examples/sec; 0.630 sec/batch)\n",
      "2016-10-10 11:57:25.100908: step 900, loss = 0.50, accuracy = 0.82, (51.9 examples/sec; 0.616 sec/batch)\n",
      "2016-10-10 11:57:31.750341: step 910, loss = 0.50, accuracy = 0.83, (52.2 examples/sec; 0.614 sec/batch)\n",
      "2016-10-10 11:57:37.721025: step 920, loss = 0.40, accuracy = 0.86, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:57:43.691992: step 930, loss = 0.43, accuracy = 0.86, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 11:57:49.660983: step 940, loss = 0.39, accuracy = 0.88, (52.6 examples/sec; 0.608 sec/batch)\n",
      "2016-10-10 11:57:55.628048: step 950, loss = 0.41, accuracy = 0.86, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:58:01.564069: step 960, loss = 0.43, accuracy = 0.86, (53.0 examples/sec; 0.604 sec/batch)\n",
      "2016-10-10 11:58:07.519713: step 970, loss = 0.34, accuracy = 0.89, (53.8 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 11:58:13.472583: step 980, loss = 0.41, accuracy = 0.86, (53.9 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 11:58:19.412895: step 990, loss = 0.33, accuracy = 0.89, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 11:58:25.383535: step 1000, loss = 0.38, accuracy = 0.87, (54.8 examples/sec; 0.584 sec/batch)\n",
      "2016-10-10 11:58:33.141258: step 1010, loss = 0.32, accuracy = 0.90, (55.9 examples/sec; 0.572 sec/batch)\n",
      "2016-10-10 11:58:39.116680: step 1020, loss = 0.33, accuracy = 0.89, (53.1 examples/sec; 0.603 sec/batch)\n",
      "2016-10-10 11:58:45.082130: step 1030, loss = 0.33, accuracy = 0.89, (53.3 examples/sec; 0.601 sec/batch)\n",
      "2016-10-10 11:58:51.059113: step 1040, loss = 0.35, accuracy = 0.87, (52.4 examples/sec; 0.610 sec/batch)\n",
      "2016-10-10 11:58:57.064151: step 1050, loss = 0.36, accuracy = 0.88, (54.2 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 11:59:03.025334: step 1060, loss = 0.35, accuracy = 0.88, (54.0 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:59:09.022039: step 1070, loss = 0.36, accuracy = 0.88, (54.0 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 11:59:14.990349: step 1080, loss = 0.36, accuracy = 0.88, (53.9 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 11:59:20.965303: step 1090, loss = 0.37, accuracy = 0.87, (51.6 examples/sec; 0.620 sec/batch)\n",
      "2016-10-10 11:59:26.953059: step 1100, loss = 0.38, accuracy = 0.87, (53.8 examples/sec; 0.595 sec/batch)\n",
      "2016-10-10 11:59:33.554317: step 1110, loss = 0.32, accuracy = 0.89, (54.0 examples/sec; 0.592 sec/batch)\n",
      "2016-10-10 11:59:39.537139: step 1120, loss = 0.29, accuracy = 0.90, (54.5 examples/sec; 0.587 sec/batch)\n",
      "2016-10-10 11:59:45.488148: step 1130, loss = 0.37, accuracy = 0.87, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 11:59:51.443995: step 1140, loss = 0.43, accuracy = 0.85, (54.2 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 11:59:57.442952: step 1150, loss = 0.35, accuracy = 0.88, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 12:00:03.412702: step 1160, loss = 0.30, accuracy = 0.90, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 12:00:09.358547: step 1170, loss = 0.30, accuracy = 0.90, (54.2 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 12:00:15.301720: step 1180, loss = 0.28, accuracy = 0.90, (53.3 examples/sec; 0.601 sec/batch)\n",
      "2016-10-10 12:00:21.254813: step 1190, loss = 0.36, accuracy = 0.87, (53.7 examples/sec; 0.596 sec/batch)\n",
      "2016-10-10 12:00:27.197243: step 1200, loss = 0.32, accuracy = 0.89, (54.3 examples/sec; 0.589 sec/batch)\n",
      "2016-10-10 12:00:33.775191: step 1210, loss = 0.33, accuracy = 0.88, (54.0 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 12:00:39.720009: step 1220, loss = 0.38, accuracy = 0.87, (54.0 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 12:00:45.552006: step 1230, loss = 0.35, accuracy = 0.87, (53.9 examples/sec; 0.594 sec/batch)\n",
      "2016-10-10 12:00:51.531051: step 1240, loss = 0.33, accuracy = 0.88, (53.1 examples/sec; 0.603 sec/batch)\n",
      "2016-10-10 12:00:57.497783: step 1250, loss = 0.36, accuracy = 0.87, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 12:01:03.452106: step 1260, loss = 0.27, accuracy = 0.91, (53.6 examples/sec; 0.597 sec/batch)\n",
      "2016-10-10 12:01:09.484277: step 1270, loss = 0.28, accuracy = 0.90, (53.2 examples/sec; 0.602 sec/batch)\n",
      "2016-10-10 12:01:15.310049: step 1280, loss = 0.36, accuracy = 0.87, (55.8 examples/sec; 0.573 sec/batch)\n",
      "2016-10-10 12:01:21.273181: step 1290, loss = 0.32, accuracy = 0.89, (53.5 examples/sec; 0.599 sec/batch)\n",
      "2016-10-10 12:01:27.279547: step 1300, loss = 0.30, accuracy = 0.89, (52.8 examples/sec; 0.606 sec/batch)\n",
      "2016-10-10 12:01:33.868193: step 1310, loss = 0.31, accuracy = 0.89, (52.1 examples/sec; 0.614 sec/batch)\n",
      "2016-10-10 12:01:39.841771: step 1320, loss = 0.31, accuracy = 0.89, (53.8 examples/sec; 0.595 sec/batch)\n",
      "2016-10-10 12:01:45.776947: step 1330, loss = 0.28, accuracy = 0.90, (54.3 examples/sec; 0.589 sec/batch)\n",
      "2016-10-10 12:01:51.727860: step 1340, loss = 0.30, accuracy = 0.89, (55.3 examples/sec; 0.579 sec/batch)\n",
      "2016-10-10 12:01:57.633277: step 1350, loss = 0.32, accuracy = 0.88, (54.2 examples/sec; 0.591 sec/batch)\n",
      "2016-10-10 12:02:03.593040: step 1360, loss = 0.33, accuracy = 0.88, (54.4 examples/sec; 0.588 sec/batch)\n",
      "2016-10-10 12:02:09.552602: step 1370, loss = 0.34, accuracy = 0.87, (54.3 examples/sec; 0.589 sec/batch)\n",
      "2016-10-10 12:02:15.500726: step 1380, loss = 0.35, accuracy = 0.87, (53.1 examples/sec; 0.603 sec/batch)\n",
      "2016-10-10 12:02:21.500974: step 1390, loss = 0.31, accuracy = 0.89, (54.2 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 12:02:27.442340: step 1400, loss = 0.29, accuracy = 0.89, (52.8 examples/sec; 0.606 sec/batch)\n",
      "2016-10-10 12:02:33.991065: step 1410, loss = 0.29, accuracy = 0.90, (54.0 examples/sec; 0.593 sec/batch)\n",
      "2016-10-10 12:02:39.933788: step 1420, loss = 0.31, accuracy = 0.89, (53.4 examples/sec; 0.599 sec/batch)\n",
      "2016-10-10 12:02:45.907666: step 1430, loss = 0.26, accuracy = 0.90, (53.2 examples/sec; 0.601 sec/batch)\n",
      "2016-10-10 12:02:51.890377: step 1440, loss = 0.24, accuracy = 0.91, (53.0 examples/sec; 0.604 sec/batch)\n",
      "2016-10-10 12:02:57.862493: step 1450, loss = 0.26, accuracy = 0.90, (54.4 examples/sec; 0.588 sec/batch)\n",
      "2016-10-10 12:03:03.827428: step 1460, loss = 0.28, accuracy = 0.89, (53.3 examples/sec; 0.600 sec/batch)\n",
      "2016-10-10 12:03:09.773189: step 1470, loss = 0.26, accuracy = 0.90, (54.3 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 12:03:15.703253: step 1480, loss = 0.25, accuracy = 0.91, (54.3 examples/sec; 0.590 sec/batch)\n",
      "2016-10-10 12:03:21.591867: step 1490, loss = 0.29, accuracy = 0.89, (54.8 examples/sec; 0.584 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
